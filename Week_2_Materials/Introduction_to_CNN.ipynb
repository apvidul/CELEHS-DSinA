{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"2f88d6c19cbe4fceaf31c7c09ec522f3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3144412595fb4f9180837c0487db07df","IPY_MODEL_20d8ebbe4215426b9088a984185fd9be","IPY_MODEL_a3b540077f0f4a70a49749e0d38122e3"],"layout":"IPY_MODEL_4476a367694b491eb345197196d343f0"}},"3144412595fb4f9180837c0487db07df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1906d1191144cc1a3ac2a750733c67f","placeholder":"​","style":"IPY_MODEL_9baf7aa84902492da0d0a7220a57411f","value":""}},"20d8ebbe4215426b9088a984185fd9be":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0b18c9897104341bf2b0c8191af0e7d","max":205615438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5dc0238e8e0345a9861bd6702f04c941","value":205615438}},"a3b540077f0f4a70a49749e0d38122e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0786d9ae89f94dc6806e055f344421fc","placeholder":"​","style":"IPY_MODEL_0ae94b09dcbb4ae8ab174af114578199","value":" 205616128/? [00:07&lt;00:00, 30651850.21it/s]"}},"4476a367694b491eb345197196d343f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1906d1191144cc1a3ac2a750733c67f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9baf7aa84902492da0d0a7220a57411f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0b18c9897104341bf2b0c8191af0e7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5dc0238e8e0345a9861bd6702f04c941":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0786d9ae89f94dc6806e055f344421fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ae94b09dcbb4ae8ab174af114578199":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/jlgrons/CELEHS-DSinA/blob/main/Week_2_Materials/Introduction_to_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","source":["# Introduction to Convolutional Neural Networks (CNN)\n","\n","This notebook guides you through an introduction to CNNs by building CNNs from scratch with the PyTorch packages using the MedMNIST dataset.\n","\n","The MedMNIST dataset is licensed under Creative Commons Attribution 4.0 International (CC BY 4.0). The dataset is from the following paper:\n","\n","* Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, Bingbing Ni. \"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification\". arXiv preprint arXiv:2110.14795, 2021.\n","* Jiancheng Yang, Rui Shi, Bingbing Ni. \"MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis\". IEEE 18th International Symposium on Biomedical Imaging (ISBI), 2021."],"metadata":{"id":"YDFe-spO5LD0"}},{"cell_type":"markdown","source":["---\n","## Initialization\n","Below, we provide the code for setting up the environment.  The first section is installing the medmnist dataset in terminal.  The second section initializes some utlitiy packages like tqdm and numpy.  The last two groups are for PyTorch and MedMNIST."],"metadata":{"id":"8ycgD_4X-k-q"}},{"cell_type":"code","source":["# medmnist installation\n","# Terminal command for installing packages. pip is commonly used to install any python package.  Here, we pass in medmnist as the package name.  You can install any existing python packages using the command format pip install XXX, where XXX is the name of the desired package.\n","!pip install medmnist\n","\n","# utlity packages\n","from tqdm import tqdm\n","import numpy as np\n","\n","# pytorch\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","import torchvision.transforms as transforms\n","\n","# medmnist python\n","import medmnist\n","from medmnist import INFO, Evaluator\n"],"metadata":{"id":"Ql_ZqidvhKzv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following is the version of MedMNIST and a link to its github page."],"metadata":{"id":"X8gAzPAv_lfC"}},{"cell_type":"code","source":["# This is a special print statement using print format.  We are outputting the version of our medmnist dataset and linking the website for the repository and data.\n","print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOCvzIjNhMVH","outputId":"d9c8aa8f-40de-40b9-db46-684498d8feae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MedMNIST v2.1.0 @ https://github.com/MedMNIST/MedMNIST/\n"]}]},{"cell_type":"markdown","source":["---\n","## Preprocessing and Parameters\n","We will be working with the PathMNIST dataset in this notebook.  This dataset involves colon pathology and was used for a study involving predicting survival from colorectal cancer histology slides.  It is here where we establish some paramters used during the model training as well as preprecessing for the data."],"metadata":{"id":"Nudy9EXAABpm"}},{"cell_type":"code","source":["# There are a lot of different dataset.  The data_flag field is that specifies which dataset we are going to be pulling from.  The other parameters are self explanatory.\n","data_flag = 'pathmnist'\n","download = True\n","\n","NUM_EPOCHS = 3\n","BATCH_SIZE = 128\n","# Learning Rate\n","lr = 0.001\n","\n","\n","info = INFO[data_flag]\n","task = info['task']\n","n_channels = info['n_channels']\n","n_classes = len(info['label'])\n","\n","# Data retrieval into a class object\n","DataClass = getattr(medmnist, info['python_class'])"],"metadata":{"id":"1YA9vn2mhN_H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This transforms our data through tensor building and normalization.\n","data_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[.5], std=[.5])\n","])\n","\n","# In these lines we split the dataset into training and testing data for future use.  These are all objects in the form of a python class.\n","train_dataset = DataClass(split='train', transform=data_transform, download=download)\n","test_dataset = DataClass(split='test', transform=data_transform, download=download)\n","\n","pil_dataset = DataClass(split='train', download=download)\n","\n","# Dataloader is simply the avenue that allows us to read in the data.  \n","train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n","test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["2f88d6c19cbe4fceaf31c7c09ec522f3","3144412595fb4f9180837c0487db07df","20d8ebbe4215426b9088a984185fd9be","a3b540077f0f4a70a49749e0d38122e3","4476a367694b491eb345197196d343f0","b1906d1191144cc1a3ac2a750733c67f","9baf7aa84902492da0d0a7220a57411f","c0b18c9897104341bf2b0c8191af0e7d","5dc0238e8e0345a9861bd6702f04c941","0786d9ae89f94dc6806e055f344421fc","0ae94b09dcbb4ae8ab174af114578199"]},"id":"nnHh2vrthP9S","outputId":"dc385d3c-f135-45f9-9b2d-c8cd888204ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://zenodo.org/record/6496656/files/pathmnist.npz?download=1 to /root/.medmnist/pathmnist.npz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/205615438 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f88d6c19cbe4fceaf31c7c09ec522f3"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n","Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n"]}]},{"cell_type":"markdown","source":["---\n","## Overview of Data\n","Printing the dataset, we can see some important information in the dataset such as the labels and their corresponding meanings, the description of the dataset, the number of channels in each image, and the task that the dataset is used for.  Following this, we print a montage of the images to gain a general understanding of what some of the images actually look like."],"metadata":{"id":"nT3lBJ6rBLSF"}},{"cell_type":"code","source":["print(train_dataset)\n","print(\"===================\")\n","print(test_dataset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zPOk06uhRi4","outputId":"7def9795-d23f-40ac-acd7-66ab900f646b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset PathMNIST (pathmnist)\n","    Number of datapoints: 89996\n","    Root location: /root/.medmnist\n","    Split: train\n","    Task: multi-class\n","    Number of channels: 3\n","    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n","    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n","    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n","    License: CC BY 4.0\n","===================\n","Dataset PathMNIST (pathmnist)\n","    Number of datapoints: 7180\n","    Root location: /root/.medmnist\n","    Split: test\n","    Task: multi-class\n","    Number of channels: 3\n","    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n","    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n","    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n","    License: CC BY 4.0\n"]}]},{"cell_type":"code","source":["# The .montage() function takes in a parameter length which allows you to set the number of images per row and per column to output in a square montage.  \n","# This function is specific to the DataClass class.  In this example, we have set the parameter length to 20, and we can count 20 images as the side length of the square montage.\n","train_dataset.montage(length=20)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"id":"YFYFhwLShhPO","outputId":"c39cdaf8-02f7-4443-bd07-bedea477446a","executionInfo":{"status":"error","timestamp":1663118736637,"user_tz":420,"elapsed":8,"user":{"displayName":"Winston Cai","userId":"08612091398649170352"}}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-8af385b3f7bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# The .montage() function takes in a parameter length which allows you to set the number of images per row and per column to output in a square montage.  This function is specific to the DataClass class.  In this example, we have set the parameter length to 20, and we can count 20 images as the side length of the square montage.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmontage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"]}]},{"cell_type":"markdown","source":["---\n","## Creating the CNN\n","In this module, we create of own CNN class object.  It will consist of 5 layers.  We also deinfe our loss funciton and optimizer based on the task at hand."],"metadata":{"id":"KBQhNWysCOne"}},{"cell_type":"code","source":["# This is the python class for our convolutional NN.  \n","\n","class Net(nn.Module):\n","    def __init__(self, in_channels, num_classes):\n","        # the super function tells us what class it inherits from.  \n","        super(Net, self).__init__()\n","\n","        # Every single one of these layers followed a similar format where we use the overarching Sequential function.  The Conv2D function specifies the type of layer that it is \n","        # and established in the number of input channels and kernel size.  BatchNorm2D is a helper function to transform our data into a normalized 2D format.  Relu is the activation \n","        # function for our layer to output to the next.  There is also MaxPool2d, which is a funciton that takes in a certain frame of the data and summarizes it by take the maximum value\n","        # within the kernel size.   Linear is also an activation function.\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(in_channels, 16, kernel_size=3),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU())\n","\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(16, 16, kernel_size=3),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(16, 64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU())\n","        \n","        self.layer4 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU())\n","\n","        self.layer5 = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2))\n","\n","        self.fc = nn.Sequential(\n","            nn.Linear(64 * 4 * 4, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, num_classes))\n","    # The forward function takes in our data and outputs the transformed the data into the classification that we want by running the data through all of the layers\n","    # that we have created.  The last layer transforms the data into the correct format that we want for classification/labeling.\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","        x = self.layer5(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","\n","# Instantiating an instance of the class that we just created.  We pass in the number of channels and the number of classes for the model.\n","model = Net(in_channels=n_channels, num_classes=n_classes)\n","    \n","# These are the loss function and optimizers that we want to deal with for this problem.  \n","if task == \"multi-label, binary-class\":\n","    criterion = nn.BCEWithLogitsLoss()\n","else:\n","    criterion = nn.CrossEntropyLoss()\n","    \n","# SGD is a stochastic gradient descent function which performs optimization for our function using the model paramters, and lr as the main function we want to optimize.  \n","# The momentum parameter is a setting that we can use to perform the gradient descent.  This is less important and more on the technical side.\n","optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n","\n"],"metadata":{"id":"dTNkwHTKhlAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Training and Testing the Model\n"],"metadata":{"id":"2dXnhJKDC6uk"}},{"cell_type":"code","source":["# We train the model using a set number of epochs\n","\n","for epoch in range(NUM_EPOCHS):\n","    # Here, we tally up the correct and total instances.\n","    train_correct = 0\n","    train_total = 0\n","    test_correct = 0\n","    test_total = 0\n","    \n","    model.train()\n","    for inputs, targets in tqdm(train_loader):\n","        # Forward training through our model, adjusting for the loss, and performing backward optimization\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        \n","        if task == 'multi-label, binary-class':\n","            targets = targets.to(torch.float32)\n","            loss = criterion(outputs, targets)\n","        else:\n","            targets = targets.squeeze().long()\n","            loss = criterion(outputs, targets)\n","        \n","        loss.backward()\n","        optimizer.step()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jO--WgHhpJt","outputId":"bf80b794-086d-4132-8108-3ae31c83b47f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 704/704 [03:09<00:00,  3.72it/s]\n","100%|██████████| 704/704 [02:58<00:00,  3.95it/s]\n","100%|██████████| 704/704 [02:56<00:00,  3.98it/s]\n"]}]},{"cell_type":"code","source":["# Evaluating the model on the testing dataset\n","\n","def test(split):\n","    # Evaluating the model\n","    model.eval()\n","    y_true = torch.tensor([])\n","    y_score = torch.tensor([])\n","    \n","    data_loader = train_loader_at_eval if split == 'train' else test_loader\n","\n","    with torch.no_grad():\n","        for inputs, targets in data_loader:\n","          # Forward training through our model, adjusting for the loss, and performing backward optimization\n","            outputs = model(inputs)\n","\n","            if task == 'multi-label, binary-class':\n","                targets = targets.to(torch.float32)\n","                outputs = outputs.softmax(dim=-1)\n","            else:\n","                targets = targets.squeeze().long()\n","                outputs = outputs.softmax(dim=-1)\n","                targets = targets.float().resize_(len(targets), 1)\n","\n","            y_true = torch.cat((y_true, targets), 0)\n","            y_score = torch.cat((y_score, outputs), 0)\n","\n","        # Transformation of data into the correct format\n","        y_true = y_true.numpy()\n","        y_score = y_score.detach().numpy()\n","        \n","        # Obtaining the performance metrics of AUC and accuracy\n","        evaluator = Evaluator(data_flag, split)\n","        metrics = evaluator.evaluate(y_score)\n","    \n","        print('%s  auc: %.3f  acc:%.3f' % (split, *metrics))\n","\n","# Calling the testing function on our traing and test datasets\n","print('==> Evaluating ...')\n","test('train')\n","test('test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ughur14Zh00Q","outputId":"444db34b-1877-4ece-b7e1-28a91d67899e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["==> Evaluating ...\n"]}]},{"cell_type":"markdown","source":["---\n","## Additional Resources\n","\n","The following module provides a different dataset in the MedMNIST corpus.\n","If you want a more detailed explainer on the compoents in a CNN, you can check the link below."],"metadata":{"id":"clfox7OUEQ14"}},{"cell_type":"code","source":["data_flag = 'organmnist3d'\n","download = True\n","\n","info = INFO[data_flag]\n","DataClass = getattr(medmnist, info['python_class'])\n","\n","# load the data\n","train_dataset = DataClass(split='train',  download=download)\n","\n","# encapsulate data into dataloader form\n","train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"],"metadata":{"id":"4jMWXPB1h5Mt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["https://poloclub.github.io/cnn-explainer/"],"metadata":{"id":"iWqyEzMCh8TR"},"execution_count":null,"outputs":[]}]}