{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANp8o9s6VcXQ"
      },
      "source": [
        "# Principal Component Analysis for Linear Dimensionality Reduction & Multi-class classification Using PathMNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te4uh--MVYgN"
      },
      "source": [
        "The number of features or variables used in a dataset is also known as the dimensionality. Often, using large number of features in the prediction can lead to complex or noisy models and introduction of unnecessary features. The process of dimensionality reduction helps in reducing the number of input features in the model, and to mitigate overfitting. Principal Component Analysis (or PCA) is one such technique for reducing dimensionality or number of features.\n",
        "\n",
        "In this exercise, we will cover an example using PathMNIST dataset (from MedMNIST (https://medmnist.com/) and use PCA to conduct dimensionality reduction. \n",
        "\n",
        "As a next step, we will also train a multi-class classification model (where the output comprises of more than two classes, as would be the case for binary classification problems)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPDEbt_YXxKV"
      },
      "source": [
        "1. First, let's install medmnist, which would give us access to the code, data and libraries created by the developers of the mednist package, to allow data science enthusiasts to develop insights, and produce predictive models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9T8WRSOV2-W"
      },
      "outputs": [],
      "source": [
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Next, we import some libraries required to run the code. We will use torch, an open-source machine learning framework, for this exercise. The input dataset **PathMNIST** is from a project called **MedMNIST**, a benchmark for lightweight biomedical image classification tasks. **PathMNIST** represents data that contains labeled examples depicting colorectal cancer. "
      ],
      "metadata": {
        "id": "4ifR_5TzU9we"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGUml_CvVR0J"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import IPython.core.display\n",
        "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "from sklearn import *\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. The data_flag field here helps pick the appropriate dataset. On MedMNIST, you will find 18 datasets, PathMNIST being one of them. "
      ],
      "metadata": {
        "id": "9YqGDYZ_6uWg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INMxvJF6Vkv8"
      },
      "outputs": [],
      "source": [
        "data_flag = 'pathmnist'\n",
        "download = True\n",
        "\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 128\n",
        "lr = 0.001\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0p3-5Q4bY1M9"
      },
      "outputs": [],
      "source": [
        "# preprocessing\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[.5], std=[.5])\n",
        "])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
        "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
        "\n",
        "pil_dataset = DataClass(split='train', download=download)\n",
        "\n",
        "# encapsulate data into dataloader form\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "train_loader_at_eval = data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcDtJIt_ZGjo"
      },
      "outputs": [],
      "source": [
        "print(train_dataset)\n",
        "print(\"===================\")\n",
        "print(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoO-VPWnZOnw"
      },
      "outputs": [],
      "source": [
        "# visualization through .montage\n",
        "\n",
        "train_dataset.montage(length=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5ejGpD6be_z"
      },
      "outputs": [],
      "source": [
        "print(type(train_dataset))\n",
        "print(type(train_dataset.imgs))\n",
        "print(train_dataset.imgs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXmXuh84cgoe"
      },
      "outputs": [],
      "source": [
        "print(train_dataset.imgs[1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(np.any(np.logical_or(train_dataset.labels == 0, train_dataset.labels == 1, train_dataset.labels == 2), axis=1))"
      ],
      "metadata": {
        "id": "6f-Uj8d7hZfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.imgs[[14]]"
      ],
      "metadata": {
        "id": "fkwMUJWlhbBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_1 = train_dataset.imgs\n",
        "test_images_1 = test_dataset.imgs\n",
        "\n",
        "train_labels_1 = train_dataset.labels\n",
        "test_labels_1 = test_dataset.labels"
      ],
      "metadata": {
        "id": "8RBH7_4Rnidl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_1"
      ],
      "metadata": {
        "id": "U8J7XRjEnomM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_1[train_labels_1 == 0] = int(0)\n",
        "train_labels_1[train_labels_1 == 1] = int(1)\n",
        "train_labels_1[train_labels_1 == 2] = int(2)\n",
        "train_labels_1[train_labels_1 == 3] = int(3)\n",
        "train_labels_1[train_labels_1 == 4] = int(4)\n",
        "train_labels_1[train_labels_1 == 5] = int(5)\n",
        "train_labels_1[train_labels_1 == 6] = int(6)\n",
        "train_labels_1[train_labels_1 == 7] = int(7)\n",
        "train_labels_1[train_labels_1 == 8] = int(8)\n",
        "\n",
        "test_labels_1[test_labels_1 == 0] = int(0)\n",
        "test_labels_1[test_labels_1 == 1] = int(1)\n",
        "test_labels_1[test_labels_1 == 2] = int(2)\n",
        "test_labels_1[test_labels_1 == 3] = int(3)\n",
        "test_labels_1[test_labels_1 == 4] = int(4)\n",
        "test_labels_1[test_labels_1 == 5] = int(5)\n",
        "test_labels_1[test_labels_1 == 6] = int(6)\n",
        "test_labels_1[test_labels_1 == 7] = int(7)\n",
        "test_labels_1[test_labels_1 == 8] = int(8)"
      ],
      "metadata": {
        "id": "zqaZs-2zjlCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the dataset\n",
        "nsamples, nx, ny, nz = train_images_1.shape\n",
        "train_images_1_reshaped = train_images_1.reshape((nsamples,nx*ny*nz))"
      ],
      "metadata": {
        "id": "_kCNpJSXkuBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the dataset for the testing dataset\n",
        "nsamples, nx, ny, nz = test_images_1.shape\n",
        "test_images_1_reshaped = test_images_1.reshape((nsamples,nx*ny*nz))"
      ],
      "metadata": {
        "id": "AhJh5FLmk4fV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_images_1_reshaped"
      ],
      "metadata": {
        "id": "CYPdvI8alBtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardize the data since PCA is highly sensitive to the scale\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "scaler.fit_transform(train_images_1_reshaped)\n",
        "\n",
        "trainX = scaler.transform(train_images_1_reshaped)\n",
        "testX = scaler.transform(test_images_1_reshaped)"
      ],
      "metadata": {
        "id": "i-WrEGjsk7oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(trainX.shape)\n",
        "print(testX.shape)"
      ],
      "metadata": {
        "id": "G5zr9Gg7qq4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the PCA\n",
        "pca = decomposition.PCA(n_components = 2)\n",
        "\n",
        "# fit the training set\n",
        "proj = pca.fit_transform(trainX)\n",
        "\n",
        "plt.scatter(proj[:, 0], proj[:, 1], c=list(map(int, train_labels_1)), cmap=plt.cm.jet, s=20)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "Pr-qy9tulTWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the percentage of explained variance\n",
        "sum(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "wwvpef6HlhyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the PCA \n",
        "# the parameter we insert is 0.9\n",
        "# this means that we select the number of components such that \n",
        "# the amount of variance that needs to be explained is greater than the percentage specified by n_components.\n",
        "pca = decomposition.PCA(0.9)\n",
        "\n",
        "# fit the training set \n",
        "trainX_90 = pca.fit_transform(trainX)"
      ],
      "metadata": {
        "id": "PPBS8HR1lmcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca.n_components_"
      ],
      "metadata": {
        "id": "vGXnyfhvlsNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_image = pca.inverse_transform(trainX_90)"
      ],
      "metadata": {
        "id": "5_XFLUkJmVdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_image.shape"
      ],
      "metadata": {
        "id": "2ED1HQoOmWj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX[6].shape"
      ],
      "metadata": {
        "id": "xJW1FvdCpmhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_image = pca.inverse_transform(trainX_90)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "\n",
        "# original image\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(trainX[6].reshape(28, 28, 3), cmap=plt.cm.gray,interpolation='nearest')\n",
        "plt.xlabel('2352 covariates')\n",
        "plt.title('Original image')\n",
        "\n",
        "# compressed image\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(compressed_image[6].reshape(28, 28, 3), cmap=plt.cm.gray, interpolation='nearest')\n",
        "plt.xlabel('185 covariates')\n",
        "plt.title('90% of explained variance')"
      ],
      "metadata": {
        "id": "qcZLbEdhlyeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ion()\n",
        "print('Label: ' + str(train_labels_1[0]))\n",
        "print('The actual image:')\n",
        "plt.imshow(train_images_1[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XDEuNCVns37T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.ion()\n",
        "print('Label: ' + str(train_labels_1[100]))\n",
        "print('The actual image:')\n",
        "plt.imshow(train_images_1[100])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0tT_fSk-tS0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_1.shape"
      ],
      "metadata": {
        "id": "jtK6asnWiDU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the dataset\n",
        "nsamples, nx, ny, nz = train_images_1.shape\n",
        "train_images_1_reshaped = train_images_1.reshape((nsamples,nx*ny*nz))"
      ],
      "metadata": {
        "id": "82oNZSJi3rEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the dataset for the testing dataset\n",
        "nsamples, nx, ny, nz = test_images_1.shape\n",
        "test_images_1_reshaped = test_images_1.reshape((nsamples,nx*ny*nz))"
      ],
      "metadata": {
        "id": "dkMGQ6Ta3s5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_1_3 = train_dataset.imgs[[i for i, x in enumerate(np.logical_or(train_dataset.labels == 0, train_dataset.labels == 1, train_dataset.labels == 2)) if x]]\n",
        "test_images_1_3 = test_dataset.imgs[[i for i, x in enumerate(np.logical_or(test_dataset.labels == 0, test_dataset.labels == 1, test_dataset.labels == 2)) if x]]\n",
        "\n",
        "train_labels_1_3 = train_dataset.labels[[i for i, x in enumerate(np.logical_or(train_dataset.labels == 0, train_dataset.labels == 1, train_dataset.labels == 2)) if x]]\n",
        "test_labels_1_3 = test_dataset.labels[[i for i, x in enumerate(np.logical_or(test_dataset.labels == 0, test_dataset.labels == 1, test_dataset.labels == 2)) if x]]"
      ],
      "metadata": {
        "id": "3lKeqwgVBCLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_1_3[train_labels_1_3 == 0] = int(0)\n",
        "train_labels_1_3[train_labels_1_3 == 1] = int(1)\n",
        "train_labels_1_3[train_labels_1_3 == 2] = int(2)\n",
        "\n",
        "test_labels_1_3[test_labels_1_3 == 0] = int(0)\n",
        "test_labels_1_3[test_labels_1_3 == 1] = int(1)\n",
        "test_labels_1_3[test_labels_1_3 == 2] = int(2)"
      ],
      "metadata": {
        "id": "VzO92cYkBYlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the dataset\n",
        "nsamples, nx, ny, nz = train_images_1_3.shape\n",
        "train_images_1_3_reshaped = train_images_1_3.reshape((nsamples,nx*ny*nz))"
      ],
      "metadata": {
        "id": "AdDNoxnlBucy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the dataset for the testing dataset\n",
        "nsamples, nx, ny, nz = test_images_1_3.shape\n",
        "test_images_1_3_reshaped = test_images_1_3.reshape((nsamples,nx*ny*nz))"
      ],
      "metadata": {
        "id": "L8z5aag-B5HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standardize the data since PCA is highly sensitive to the scale\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "scaler.fit_transform(train_images_1_3_reshaped)\n",
        "\n",
        "trainX = scaler.transform(train_images_1_3_reshaped)\n",
        "testX = scaler.transform(test_images_1_3_reshaped)"
      ],
      "metadata": {
        "id": "QZS17zreB_k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time \n",
        "\n",
        "start = time.time()\n",
        "logreg = linear_model.LogisticRegression(solver = 'lbfgs', max_iter = 10)\n",
        "logreg.fit(trainX, train_labels_1_3)\n",
        "stop = time.time()\n",
        "\n",
        "print(\"training time  =\", stop - start)\n",
        "\n",
        "# predict from the model\n",
        "predYtest  = logreg.predict(testX)\n",
        "\n",
        "# calculate accuracy for testing set\n",
        "acc      = metrics.accuracy_score(test_labels_1_3, predYtest)\n",
        "print(\"test accuracy  =\", acc)"
      ],
      "metadata": {
        "id": "nwSGCyx_AkTR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Introduction_to_PCA_MC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}