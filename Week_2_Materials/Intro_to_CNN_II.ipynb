{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"widgets":{"application/vnd.jupyter.widget-state+json":{"91692f661a5947769c5d07a71e7ee963":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d39e58d4b84f469bb844fb5774744a9a","IPY_MODEL_ccd973c133ae464095c316a41cf9f334","IPY_MODEL_88d73d9431fc470a93173c5de9fd23a9"],"layout":"IPY_MODEL_a0189891e27f4ad2875e37daa01fe31c"}},"d39e58d4b84f469bb844fb5774744a9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7e6cb16860c4565a96d99a09eb4a85c","placeholder":"​","style":"IPY_MODEL_ff6e0ff94ada48568a5069f86556eff2","value":""}},"ccd973c133ae464095c316a41cf9f334":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d41e41bcad4a43b3af997cb64ea24b7d","max":205615438,"min":0,"orientation":"horizontal","style":"IPY_MODEL_900090e8c68a4f7db7f2b42b7e7b50d5","value":205615438}},"88d73d9431fc470a93173c5de9fd23a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de146224bcd74d87a162f957ddf201f6","placeholder":"​","style":"IPY_MODEL_a7f04eac20484770984197b1327f7989","value":" 205616128/? [00:53&lt;00:00, 5377933.38it/s]"}},"a0189891e27f4ad2875e37daa01fe31c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7e6cb16860c4565a96d99a09eb4a85c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff6e0ff94ada48568a5069f86556eff2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d41e41bcad4a43b3af997cb64ea24b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"900090e8c68a4f7db7f2b42b7e7b50d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"de146224bcd74d87a162f957ddf201f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7f04eac20484770984197b1327f7989":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/jlgrons/CELEHS-DSinA/blob/main/Intro_to_CNN_II.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"o2nUCqQ-hfZR","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"d4e27552-98e7-4e14-d18e-a8f44911eb98"},"source":["import tensorflow as tf\n","import keras\n","keras.__version__"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'2.8.0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Wb7PAQLnhfZW"},"source":["# 5.1 - Introduction to Convolutional Neural Networks (CNN)\n","\n","This notebook contains part of a python tutorial found in chapter 5, section 1 of [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python?a_aid=keras&a_bid=76564dff). In the book you will find the commented code with more detailed explanations of this and other examples, databases and CNN architectures.\n"]},{"cell_type":"markdown","metadata":{"id":"GJjhMgcPaH6o"},"source":["----\n","##Background\n","We will first go over a simple CNN example. Again we will focus on classifying images of hand written digits from the MNIST dataset.\n","\n","The 6 lines of code in the cell below will show you what a basic CNN looks like. It is a stack of 2-dimensional convolutional `Conv2D` and pooling `MaxPooling2D` layers. We will see what each does in practice in the cells below. As we have seen in the course slides, CNNs will have tensors of dimension `(image_height, image_width, image_channels)` as inputs. In our case, we will configure the network to take tensors of dimension `(28, 28, 1)` (note the MNIST image format). \n","\n","We will do this in a similar manner as what we did for the simple neural networks, adding `input_shape=(28, 28, 1)` to the first layer."]},{"cell_type":"markdown","metadata":{"id":"EBqaSBQqwchE"},"source":["---\n","First we will import the necessary modules from keras and load the MNIST dataset."]},{"cell_type":"code","metadata":{"id":"EYudrJybhfZW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b4086fa0-59a2-4e52-8d42-7837b7b8e0e4"},"source":["# medmnist and keras installation\n","# Terminal command for installing packages. pip is commonly used to install any python package.  Here, we pass in medmnist as the package name.  You can install any existing python packages using the command format pip install XXX, where XXX is the name of the desired package.\n","! pip install medmnist\n","! pip install keras\n","\n","# keras\n","from keras import layers\n","from keras import models\n","from keras.datasets import mnist\n","from tensorflow.keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from tensorflow.keras.optimizers import SGD\n","import torchvision.transforms as transforms\n","\n","# utility packages\n","import numpy as np\n","\n","# medmnist python\n","import medmnist\n","from medmnist import INFO, Evaluator"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting medmnist\n","  Downloading medmnist-2.1.0-py3-none-any.whl (21 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from medmnist) (4.64.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.21.6)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.11.0+cu113)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.0.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from medmnist) (7.1.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from medmnist) (0.12.0+cu113)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from medmnist) (1.3.5)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from medmnist) (0.18.3)\n","Collecting fire\n","  Downloading fire-0.4.0.tar.gz (87 kB)\n","\u001b[K     |████████████████████████████████| 87 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire->medmnist) (1.15.0)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->medmnist) (1.1.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->medmnist) (2022.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->medmnist) (2.8.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2.4.1)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (1.4.1)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (1.3.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (3.2.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2.6.3)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image->medmnist) (2021.11.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (1.4.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->medmnist) (4.1.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->medmnist) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->medmnist) (1.1.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->medmnist) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->medmnist) (2022.6.15)\n","Building wheels for collected packages: fire\n","  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=add89912860536b12bf94d687d7b9c385707dcd4fcd45fda94c42d6264c50440\n","  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n","Successfully built fire\n","Installing collected packages: fire, medmnist\n","Successfully installed fire-0.4.0 medmnist-2.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"]}]},{"cell_type":"code","metadata":{"id":"61aK5BJn6a2V","colab":{"base_uri":"https://localhost:8080/","height":295,"referenced_widgets":["91692f661a5947769c5d07a71e7ee963","d39e58d4b84f469bb844fb5774744a9a","ccd973c133ae464095c316a41cf9f334","88d73d9431fc470a93173c5de9fd23a9","a0189891e27f4ad2875e37daa01fe31c","f7e6cb16860c4565a96d99a09eb4a85c","ff6e0ff94ada48568a5069f86556eff2","d41e41bcad4a43b3af997cb64ea24b7d","900090e8c68a4f7db7f2b42b7e7b50d5","de146224bcd74d87a162f957ddf201f6","a7f04eac20484770984197b1327f7989"]},"outputId":"11dca897-abbd-48c3-eb1f-00de00da6005"},"source":["# Helper function to load in the relevant MedMNist dataset\n","def get_loader(dataset, batch_size):\n","    total_size = len(dataset)\n","    print('Size', total_size)\n","    index_generator = shuffle_iterator(range(total_size))\n","    while True:\n","        data = []\n","        for _ in range(batch_size):\n","            idx = next(index_generator)\n","            data.append(dataset[idx])\n","        yield dataset._collate_fn(data)\n","\n","# Shuffling the items in the iterator\n","def shuffle_iterator(iterator):\n","    # iterator should have limited size\n","    index = list(iterator)\n","    total_size = len(index)\n","    i = 0\n","    random.shuffle(index)\n","    while True:\n","        yield index[i]\n","        i += 1\n","        if i >= total_size:\n","            i = 0\n","            random.shuffle(index)\n","\n","# Converts an rgb value into the corresponding grayscale\n","def rgb2gray(rgb):\n","    R, G, B = rgb[0], rgb[1], rgb[2]\n","    imgGray = 0.2989 * R + 0.5870 * G + 0.1140 * B\n","    return imgGray\n","\n","# There are a lot of different dataset.  The data_flag field is that specifies which dataset we are going to be pulling from.  The other parameters are self explanatory.\n","data_flag = 'pathmnist'\n","download = True\n","\n","NUM_EPOCHS = 3\n","BATCH_SIZE = 64\n","\n","info = INFO[data_flag]\n","task = info['task']\n","n_channels = info['n_channels']\n","n_classes = len(info['label'])\n","\n","DataClass = getattr(medmnist, info['python_class'])\n","\n","# This transforms our data through tensor building and normalization.\n","data_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[.5], std=[.5])\n","])\n","\n","# In these lines we split the dataset into training and testing data for future use.  These are all objects in the form of a python class.\n","train_dataset = DataClass(split='train', transform=data_transform, download=download)\n","test_dataset = DataClass(split='test', transform=data_transform, download=download)\n","\n","pil_dataset = DataClass(split='train', download=download)\n","\n","# Dataloader is simply the avenue that allows us to read in the data.  \n","train_loader = get_loader(dataset=train_dataset, batch_size=BATCH_SIZE)\n","test_loader = get_loader(dataset=test_dataset, batch_size=BATCH_SIZE)\n","\n","print(train_dataset)\n","\n","# Transforming the data into gray scale and saving the images and labels separatly for each folder\n","training_images = np.array([np.array(rgb2gray(elem[0])) for elem in train_dataset])\n","training_labels = np.array([np.array(elem[1]) for elem in train_dataset])\n","testing_images = np.array([np.array(rgb2gray(elem[0])) for elem in test_dataset])\n","testing_labels = np.array([np.array(elem[1]) for elem in test_dataset])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://zenodo.org/record/6496656/files/pathmnist.npz?download=1 to /root/.medmnist/pathmnist.npz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/205615438 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91692f661a5947769c5d07a71e7ee963"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n","Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n","Dataset PathMNIST (pathmnist)\n","    Number of datapoints: 89996\n","    Root location: /root/.medmnist\n","    Split: train\n","    Task: multi-class\n","    Number of channels: 3\n","    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n","    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n","    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n","    License: CC BY 4.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"TXaNVH_UaT7N"},"source":["---\n","##Building your CNN"]},{"cell_type":"markdown","metadata":{"id":"GEBEgWeHjYWG"},"source":["###Construction\n","Next, build a CNN that takes as input MNIST images, and has:\n","\n","1. 2D convolutional layer with 32 units, size 3x3, and [`relu`](https://keras.io/api/layers/activations/) activation function (HINT: function [`Conv2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D); note input shape above)\n","\n","2. 2D max pooling layer with units size 2x2 (HINT: function [`MaxPooling2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D))\n","\n","3. 2D convolutional layer with 64 units, size 3x3, and `relu` activation function\n","\n","4. 2D max pooling layer with units size 2x2\n","\n","5. 2D convolutional layer with 64 units, size 3x3, and `relu` activation function\n"]},{"cell_type":"code","metadata":{"id":"g8gM_38aIDEW","colab":{"base_uri":"https://localhost:8080/","height":381},"outputId":"657354fd-e1f8-4ca5-ef52-2735da9f27e6"},"source":["########## STUDENT WORK ##########\n","# Build the model\n"," \n","CNN = models.Sequential()\n","# Fill in the parentheses below with the relevant code\n","CNN.add()\n","CNN.add()\n","CNN.add()\n","CNN.add()\n","CNN.add()\n","CNN.summary()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e6dd7a21a453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Fill in the parentheses below with the relevant code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: add() missing 1 required positional argument: 'layer'"]}]},{"cell_type":"code","metadata":{"id":"7tcjtFYUmt_y"},"source":["###### DO NOT RUN THIS CELL ############\n","# expected result\n","\n","#Model: \"sequential\"\n","#_________________________________________________________________\n","#Layer (type)                 Output Shape              Param #   \n","#=================================================================\n","#conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","#_________________________________________________________________\n","#max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","#_________________________________________________________________\n","#conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","#_________________________________________________________________\n","#max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","#_________________________________________________________________\n","#conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","#=================================================================\n","#Total params: 55,744\n","#Trainable params: 55,744\n","#Non-trainable params: 0\n","#_________________________________________________________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOTJkSrhhfZd"},"source":["In the table above, you can see that the result of each convolutional layer (`Conv2D`) and each max pooling layer (`MaxPooling2D`) is a 3D tensor (a three dimensional matrix) of dimensions: `(hight, width, channels)`. The height and width dimensions tend to decrease as we get deeper in the neural network. The number of channels is determined by the first argument in the convolutional layer function `Conv2D` (i.e. 32 or 64).\n","\n","---\n","The next step will be to feed the last generated tensor (of size `(3, 3, 64)`) into the fully connected classifying neural network: a stack of dense layers (`Dense`). However, the vectors used for classifying are one-dimensional, as opposed to our 3D tensor; we thus need to \"flatten\" ([`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)) the 3D tensors (i.e. transform them into vectors) and then add a few dense layers on top.\n","\n","We will be doing a 10 category classification task (digits from 0 to 9). What size should the last layer be? What activation function should it have? (Hint: think back to the python notebook on simple neural networks)"]},{"cell_type":"markdown","metadata":{"id":"XLfnjL1ZjYWT"},"source":["Extend your CNN by adding:\n","\n","1. A layer to flatten the 3D tensor into a vector \n","\n","2. A dense layer with 64 units (neurons) and `relu` activation function \n","\n","3. A dense layer with what number of units? Which activation function do we need for this one?\n"]},{"cell_type":"code","metadata":{"id":"vgNVNjddpQeE"},"source":["########## STUDENT WORK ##########\n","\n","CNN.add()\n","# Fill in the parentheses below with the appropriate code\n","CNN.add()\n","CNN.add()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xCHNnLRuJHRZ"},"source":["CNN.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RN5hhNbbhfZg"},"source":["##### DO NOT RUN THIS CELL ####\n","# expected output\n","\n","#Model: \"sequential\"\n","#_________________________________________________________________\n","#Layer (type)                 Output Shape              Param #   \n","#=================================================================\n","#conv2d (Conv2D)              (None, 26, 26, 32)        320       \n","#_________________________________________________________________\n","#max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n","#_________________________________________________________________\n","#conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n","#_________________________________________________________________\n","#max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n","#_________________________________________________________________\n","#conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n","#_________________________________________________________________\n","#flatten (Flatten)            (None, 576)               0         \n","#_________________________________________________________________\n","#dense (Dense)                (None, 64)                36928     \n","#_________________________________________________________________\n","#dense_1 (Dense)              (None, 10)                650       \n","#=================================================================\n","#Total params: 93,322\n","#Trainable params: 93,322\n","#Non-trainable params: 0\n","#_________________________________________________________________"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqrk4FryTUDx"},"source":["As you can see, our output tensor of dimension `(3, 3, 64)` was flattened into a vector of size (3x3x6,)= `(576,)` before being processed by the two dense layers. Now that we've finished with our architecture, we will train the CNN on the MNIST data.\n"]},{"cell_type":"markdown","metadata":{"id":"3XPJEgL9WB7W"},"source":["---\n","###Testing the Data Labels\n","We use 60,000 samples for training the neural network and 10,00 to see how well it performs. The images we will be classifying look like the image below, you can change the parameter `i` to see different examples of images their corresponding labels:"]},{"cell_type":"code","metadata":{"id":"sONSorDZV8yT"},"source":["########## STUDENT WORK ##########\n","#play around with this!\n","\n","from matplotlib import pyplot as plt\n","\n","# Select the ith image in the dataset to print\n","i=300\n","\n","# Plotting the image\n","print('label: ' + str(training_labels[i]))\n","# Subsetting the image from the dataset\n","plt.imshow(training_images[i])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4yMp1PfqdqL"},"source":["---\n","###Processing the Data\n","Before beginning the training, you will process the images. The matrices must be converted into vectors as this is what the network takes as inputs. Additionally, the tones of gray must be normalized so that they are in the interval [0,1]. Currently, there are 60,000 28x28 matrices with values between [0,255], and you need to convert each one to a vector of 28*28 = 784 entries."]},{"cell_type":"code","metadata":{"id":"cRig2dO4-7f3"},"source":["# Normalizing the images \n","training_images = training_images.reshape(training_images.shape[0], 28, 28, 1).astype('float32')\n","testing_images = testing_images.reshape(testing_images.shape[0], 28, 28, 1).astype('float32')\n","\n","training_images = training_images / 255\n","testing_images = testing_images / 255"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0kcegnosAMa"},"source":["We'll use the `RMSprop` optimizer for the data:"]},{"cell_type":"code","metadata":{"id":"73VZPx1O_eF9"},"source":["########## STUDENT WORK ##########\n","# compile the model\n","# hint: the optimizer is in the text above\n","# hint: the loss is commonly used for multi-class\n","\n","CNN.compile(optimizer='', \n","              loss='',  \n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h6rrBBEQbJLW"},"source":["###Training the Model"]},{"cell_type":"code","metadata":{"id":"AHKbl6AehfZl"},"source":["# Fitting the model using 5 epochs and a batch size of 64\n","history = CNN.fit(training_images, training_labels, epochs=5, batch_size=64)\n","history"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkiDxTX9AJvB"},"source":["---\n","##Visualizing the Training Results"]},{"cell_type":"markdown","metadata":{"id":"crSUluKIvL8s"},"source":["To visualize the training results, we will consider both the loss and accuracy during training as shown below."]},{"cell_type":"code","metadata":{"id":"0Vhq1s_mAF3u"},"source":["# the training results in the formation of a dictionary\n","history.history.keys() #view the history dictionary keys"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4UB-FGXsvgUO"},"source":["---\n","First, we will plot the loss vs. epoch graph. \n","\n","Fill in the correct history dictionary key below:"]},{"cell_type":"code","metadata":{"id":"WmbEyZI0ANM7"},"source":["import matplotlib.pyplot as plt\n","\n","# history is a variable that stores the results of the trained model\n","# .history is an dictionary attribute of the results of the trained model \n","# the history dictionary contains the losses and accuracy levels per epoch\n","\n","\n","########## STUDENT WORK ##########\n","# put in one of the history dictionary keys in the brackets\n","loss = history.history[]\n","\n","\n","epochs = range(1,len(loss)+1)\n","\n","# Time to plot the graph\n","plt.plot(epochs, loss, 'b', label = 'Training loss')\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlqI3MdBvzOj"},"source":["What do we notice about the loss as epoch increases? Is this what we expect?"]},{"cell_type":"markdown","metadata":{"id":"DbMBtkMhwAAW"},"source":["```\n","STUDENT'S RESPONSE\n","```"]},{"cell_type":"markdown","metadata":{"id":"eJQQ6MLCv9-c"},"source":["---\n","Next, we will plot the accuracy vs. epoch graph.\n","\n","Fill in the correct history dictionary key below:"]},{"cell_type":"code","metadata":{"id":"u2NTX_RoAR7J"},"source":["########## STUDENT WORK ##########\n","# put in one of the history dictionary keys in the brackets\n","accuracy = history.history[]\n","\n","plt.plot(epochs, accuracy, 'b', label =\"Train accuracy\")\n","plt.title(\"Training Accuracy\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gPNGx8PqwmIv"},"source":["What do we notice about the loss as epoch increases? Is this what we expect?"]},{"cell_type":"markdown","metadata":{"id":"5lCBcLVLwpL4"},"source":["```\n","STUDENT'S RESPONSE\n","```"]},{"cell_type":"markdown","metadata":{"id":"rXrB_-P8AlPx"},"source":["---\n","##Discussion of Loss and Accuracy\n","Finally, we will compare testing and training accuracy."]},{"cell_type":"code","metadata":{"id":"ouSzjnpxAk-3"},"source":["train_loss, train_acc = CNN.evaluate(training_images, training_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SA_37rFnAu4k"},"source":["print(\"Training accuracy: {0:.1%}\".format(train_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V69z_AGNA0xg"},"source":["# Evaluating the performance of the model using the testing data. \n","testing_loss, testing_acc = CNN.evaluate(testing_images, testing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_K-1YgNbBCjE"},"source":["print(\"Testing accuracy: {0:.1%}\".format(testing_acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b3Tny1bqw7cy"},"source":["Compare the training and testing accuracy. Is this what we expect? Why?"]},{"cell_type":"markdown","metadata":{"id":"RIq3BsoUxGQq"},"source":["```\n","STUDENT'S RESPONSE\n","```"]}]}